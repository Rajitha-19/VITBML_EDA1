{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment 1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajithaponduru/VITBML_EDA1/blob/master/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXM1P_NKpsbd",
        "colab_type": "text"
      },
      "source": [
        "# NLP Assignment\n",
        "In this assignment you will learn to do some data cleaning, prepare your features and do some K Means clustering on the data.\n",
        "\n",
        "[Download data from here](https://drive.google.com/open?id=1ozz0zITDG8wOSQtwop2NC7AM_T3O59nj)\n",
        "\n",
        "Load it as a pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl3ScdoUpQUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-wjqgzfqNgj",
        "colab_type": "text"
      },
      "source": [
        "We are interested in customer_reviews column. However, the column is not clean data. It contains review title, rating, date, customer name, and review all in one scell separated by //\n",
        "\n",
        "[Read this tutorial](https://www.geeksforgeeks.org/python-pandas-split-strings-into-two-list-columns-using-str-split/)\n",
        "\n",
        "Extract the customer reviews in another dataframe and separate each of the components\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4uL5_5aqKfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K8RZb1JqXxp",
        "colab_type": "text"
      },
      "source": [
        "Now the customer name column needs to be cleaned. Extract only the customer name and remove the By and on part in similar manner. This time you need to split by '\\n'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsxPN-uUqXPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ-aUKqeqnHq",
        "colab_type": "text"
      },
      "source": [
        "Create another column nlpreview and copy the review column as it is\n",
        "apply removal of punctuation and extract all words as a list of words. \n",
        "\n",
        "**Hint**: You may have to use [apply method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) on dataframe series with a lambda function that does your tokenization etc. You can use sentence/word tokenizer but you will get extra credit if you use RegExpTokenizer. [Documentation for help](https://kite.com/python/docs/nltk.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_4fIVr0rVgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_gzdqFxrWyW",
        "colab_type": "text"
      },
      "source": [
        "Remove stopwords in the nlpreview column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZtMwBlurZJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwbA0yArc8o",
        "colab_type": "text"
      },
      "source": [
        "Use stemming/lemmatization. There are several options, so [read the docs](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms9UsQcjseWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35qEoH5QsN68",
        "colab_type": "text"
      },
      "source": [
        "Find the first 100 most important words, i.e. most frequently used words and plot them using FreqDist plot in nltk. See the sample colab file given during class. Note your observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CgoOWcNr3a9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zIyAU3br2wx",
        "colab_type": "text"
      },
      "source": [
        "Create another column called TFIDF and use the nlpreview to construct tfidf in this newly created column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NxDlX_Osgyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSd6jz2YskxQ",
        "colab_type": "text"
      },
      "source": [
        "Find the top 100 words with maximum TF IDF score and plot it. Note your observations and see how different it is from the Top 100 highest frequency words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2t-S1P5tCUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unOAXitFtDp6",
        "colab_type": "text"
      },
      "source": [
        "Use KMeans clustering to apply clustering to this TFIDF column. Find optimal number of clusters using elbow plot and note your observations for the optimal clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3VeNhlhtUe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQvXpTJnI5LP",
        "colab_type": "text"
      },
      "source": [
        "# visualize your clusters\n",
        " Let us use word cloud visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAH1qQV8I4fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt \n",
        "def plotwordcloud(words):\n",
        "  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words) \n",
        "  plt.figure(figsize=(10, 7)) \n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
        "  plt.axis('off') \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_81bmQX2JMYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here to call plotwordcloud for concatenated word set of each cluster. You need to find all words of a cluster and create a set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgAS3ungH5Tn",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Analysis\n",
        "Ideally it is difficult to do sentiment analysis without having labeled data.\n",
        "\n",
        "We can do following:\n",
        "1. Use the rating column and rate 1,2 as negative and 4,5 as positive and reject 3 for now. Then we do 70/30 split of our data into train/test set \n",
        "2. Use a lexicon of positive and negative words and calculate score of each review using high score for positive and low score for negative review and pick a threshold above which a review is positive. We can find known positive or negative words in a review and sum their scores up to get a final score.\n",
        "\n",
        "However, note that text is tricky:\n",
        "\n",
        "E.g. consider a sentence in review of a smart watch: *The product is not only good but also does not spoil in water.*\n",
        "\n",
        "It is a positive review but has lot of negaive words not, spoil etc. In these cases one has to go for bi or tri grams, e.g. *not only good* is a positive trigram. *does not spoil* is another one. However adding trigrams make your problem tough. If a review has 10 words, it will have 7 trigrams and 8 bigrams in it, making the features more.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93_GT34eOX4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a column sentiment with 0 meaning negative and 1 meaning positive and construct this based on the rule in point 1 above."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtIjQYwAPxBz",
        "colab_type": "text"
      },
      "source": [
        "Read [this stack overflow post](https://stackoverflow.com/questions/47778403/computing-tf-idf-on-the-whole-dataset-or-only-on-training-data). It tells you how to create you train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MG8NhOPOsEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle and split your data in train test (70/30) with tfidf as your features and sentiment as your labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9evZRPeQMG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot word clourds for positive and negative training examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X1gPIlIO1qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use and supervised classification approach of your choice and report your accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfuipPcaPHET",
        "colab_type": "text"
      },
      "source": [
        "Evaluate your model by trying your own reviews.\n",
        "\n",
        "Process will be:\n",
        "1. Calculate tf-idf of the review using [transform() method](https://stackoverflow.com/questions/47778403/computing-tf-idf-on-the-whole-dataset-or-only-on-training-data)\n",
        "2. run your model on the tf-idf vector and see the sentiment predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZl-4_4O_Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try your model on new data ie. your own cooked up data say some review that you write"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0hETGM9QS5J",
        "colab_type": "text"
      },
      "source": [
        "# Approach 2 - Lexicon way\n",
        "We will try the lexicons. Rather than doing the work on our own we shall use built in lexicons in NLTK.\n",
        "You can [read about vader here](http://datameetsmedia.com/staging/3908/vader-sentiment-analysis-explained/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pKCRViTQ2_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiIcW7Z0RAAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "scores = []\n",
        "# for each review in your dataframe (here we need full review and not bother about removing stopwords, punctuations etc. In fact vader needs them!)\n",
        "# your loop statement e.g., for sentence in ????\n",
        "     #print(sentence)\n",
        "     sentiment_score = sentiment_analyzer.polarity_scores(sentence)\n",
        "     scores.append(sentiment_score)\n",
        "\n",
        "# let us see first 10 results. You can change this code to see more.\n",
        "for sentiment_score in scores[:10]:\n",
        "  for score in sentiment_score:\n",
        "         print('{0}: {1},' .format(score, sentiment_score[score]), end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLuyMquVRb0D",
        "colab_type": "text"
      },
      "source": [
        "You can analyze the outcomes. See what threshold you want to pick!"
      ]
    }
  ]
}